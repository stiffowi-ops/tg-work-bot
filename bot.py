import os
import json
import random
import logging
import asyncio
import aiohttp
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
import pytz
from bs4 import BeautifulSoup
from fake_useragent import UserAgent  # pip install fake-useragent

from telegram import Update
from telegram.constants import ParseMode
from telegram.ext import (
    Application,
    CommandHandler,
    ContextTypes,
    JobQueue,
)

# ========== –ù–ê–°–¢–†–û–ô–ö–ò ==========
TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CONFIG_FILE = "bot_digest.json"

# –í—Ä–µ–º—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞ (12:00 –ø–æ –ú–æ—Å–∫–≤–µ)
DIGEST_TIME = {"hour": 12, "minute": 0}
TIMEZONE = pytz.timezone("Europe/Moscow")
DIGEST_DAYS = [0, 1, 2, 3, 4]  # –ü–Ω-–ü—Ç

# ========== –ù–ê–°–¢–†–û–ô–ö–ò –ü–ê–†–°–ò–ù–ì–ê ==========
USER_AGENT = UserAgent()

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
NEWS_SOURCES = {
    '—Å–ø–æ—Ä—Ç': [
        {
            'name': '–°–ø–æ—Ä—Ç-–≠–∫—Å–ø—Ä–µ—Å—Å',
            'url': 'https://www.sport-express.ru/services/materials/news/last/',
            'parser': 'parse_sportexpress'
        },
        {
            'name': '–ß–µ–º–ø–∏–æ–Ω–∞—Ç',
            'url': 'https://www.championat.com/news/1.html',
            'parser': 'parse_championat'
        },
        {
            'name': '–ú–∞—Ç—á –¢–í',
            'url': 'https://matchtv.ru/news/',
            'parser': 'parse_matchtv'
        }
    ],
    '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': [
        {
            'name': '–•–∞–±—Ä–∞—Ö–∞–±—Ä',
            'url': 'https://habr.com/ru/news/',
            'parser': 'parse_habr'
        },
        {
            'name': 'VC.ru',
            'url': 'https://vc.ru/new',
            'parser': 'parse_vc'
        },
        {
            'name': 'TJ',
            'url': 'https://tjournal.ru/news',
            'parser': 'parse_tjournal'
        }
    ],
    '–∫—É—Ä—å—ë–∑—ã': [
        {
            'name': '–ö–æ–º—Å–æ–º–æ–ª—å—Å–∫–∞—è –ø—Ä–∞–≤–¥–∞',
            'url': 'https://www.kp.ru/online/news/',
            'parser': 'parse_kp'
        },
        {
            'name': '–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏',
            'url': 'https://ria.ru/incidents/',
            'parser': 'parse_ria'
        },
        {
            'name': 'Lenta.ru',
            'url': 'https://lenta.ru/rubrics/culture/curious/',
            'parser': 'parse_lenta'
        }
    ]
}

# –ì–æ—Ä–æ–¥–∞ –¥–ª—è –ø–æ–≥–æ–¥—ã
CITIES = [
    {"name": "–ú–æ—Å–∫–≤–∞", "yandex_code": "moscow"},
    {"name": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "yandex_code": "saint-petersburg"},
    {"name": "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "yandex_code": "novosibirsk"},
    {"name": "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", "yandex_code": "yekaterinburg"},
    {"name": "–ö–∞–∑–∞–Ω—å", "yandex_code": "kazan"}
]

# ========== –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ==========
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", 
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# ========== –ö–õ–ê–°–° –î–õ–Ø –ü–ê–†–°–ò–ù–ì–ê ==========
class NewsWeatherParser:
    """–ü–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –ø–æ–≥–æ–¥—ã"""
    
    def __init__(self):
        self.session = None
        self.news_cache = {}
        self.weather_cache = {}
        self.cache_timeout = 1800  # 30 –º–∏–Ω—É—Ç
        
    async def init_session(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏"""
        if not self.session:
            self.session = aiohttp.ClientSession(
                headers={'User-Agent': USER_AGENT.random},
                timeout=aiohttp.ClientTimeout(total=10)
            )
    
    async def close_session(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()
            self.session = None
    
    # ========== –ü–ê–†–°–ò–ù–ì –ù–û–í–û–°–¢–ï–ô ==========
    async def get_news_by_category(self, category: str, count: int = 1) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        cache_key = f"news_{category}_{datetime.now().strftime('%Y%m%d%H')}"
        
        if cache_key in self.news_cache:
            cached_time, data = self.news_cache[cache_key]
            if (datetime.now() - cached_time).seconds < self.cache_timeout:
                return random.sample(data, min(len(data), count))
        
        all_news = []
        sources = NEWS_SOURCES.get(category, [])
        
        for source in sources:
            try:
                news = await getattr(self, source['parser'])(source['url'])
                for item in news:
                    item['source'] = source['name']
                all_news.extend(news[:3])  # –ë–µ—Ä–µ–º –ø–æ 3 –Ω–æ–≤–æ—Å—Ç–∏ —Å –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                await asyncio.sleep(0.5)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source['name']}: {e}")
                continue
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self.news_cache[cache_key] = (datetime.now(), all_news)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        if len(all_news) > count:
            return random.sample(all_news, count)
        return all_news
    
    # –ü–∞—Ä—Å–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
    async def parse_sportexpress(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –°–ø–æ—Ä—Ç-–≠–∫—Å–ø—Ä–µ—Å—Å"""
        async with self.session.get(url) as response:
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            news_items = []
            for article in soup.find_all('div', class_='se-material__title', limit=10):
                title_elem = article.find('a')
                if title_elem:
                    link_elem = title_elem.get('href')
                    if link_elem and not link_elem.startswith('http'):
                        link_elem = 'https://www.sport-express.ru' + link_elem
                    
                    news_items.append({
                        'title': title_elem.text.strip(),
                        'url': link_elem,
                        'description': self._generate_description(title_elem.text.strip())
                    })
            
            return news_items
    
    async def parse_championat(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ß–µ–º–ø–∏–æ–Ω–∞—Ç"""
        async with self.session.get(url) as response:
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            news_items = []
            for article in soup.find_all('a', class_='news-item__title', limit=10):
                title = article.text.strip()
                link = article.get('href')
                if not link.startswith('http'):
                    link = 'https://www.championat.com' + link
                
                news_items.append({
                    'title': title,
                    'url': link,
                    'description': self._generate_description(title)
                })
            
            return news_items
    
    async def parse_habr(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –•–∞–±—Ä–∞—Ö–∞–±—Ä"""
        async with self.session.get(url) as response:
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            news_items = []
            for article in soup.find_all('article', class_='tm-articles-list__item', limit=10):
                title_elem = article.find('h2', class_='tm-title')
                if title_elem:
                    link_elem = title_elem.find('a')
                    if link_elem:
                        title = link_elem.text.strip()
                        link = 'https://habr.com' + link_elem.get('href')
                        
                        # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                        desc_elem = article.find('div', class_='tm-article-body tm-article-snippet__lead')
                        description = desc_elem.text.strip()[:150] + '...' if desc_elem else self._generate_description(title)
                        
                        news_items.append({
                            'title': title,
                            'url': link,
                            'description': description
                        })
            
            return news_items
    
    async def parse_vc(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ VC.ru"""
        async with self.session.get(url) as response:
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            news_items = []
            for article in soup.find_all('div', class_='content-container', limit=10):
                title_elem = article.find('a', class_='content-title')
                if title_elem:
                    title = title_elem.text.strip()
                    link = title_elem.get('href')
                    if not link.startswith('http'):
                        link = 'https://vc.ru' + link
                    
                    # –û–ø–∏—Å–∞–Ω–∏–µ
                    desc_elem = article.find('div', class_='content-description')
                    description = desc_elem.text.strip()[:150] + '...' if desc_elem else self._generate_description(title)
                    
                    news_items.append({
                        'title': title,
                        'url': link,
                        'description': description
                    })
            
            return news_items
    
    async def parse_kp(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ö–æ–º—Å–æ–º–æ–ª—å—Å–∫–∞—è –ø—Ä–∞–≤–¥–∞"""
        async with self.session.get(url) as response:
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            news_items = []
            for article in soup.find_all('div', class_='sc-12iwwi7', limit=10):
                title_elem = article.find('a')
                if title_elem:
                    title = title_elem.text.strip()
                    link = title_elem.get('href')
                    if link and not link.startswith('http'):
                        link = 'https://www.kp.ru' + link
                    
                    news_items.append({
                        'title': title,
                        'url': link,
                        'description': self._generate_description(title)
                    })
            
            return news_items
    
    async def parse_ria(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏"""
        async with self.session.get(url) as response:
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            news_items = []
            for article in soup.find_all('div', class_='list-item', limit=10):
                title_elem = article.find('a', class_='list-item__title')
                if title_elem:
                    title = title_elem.text.strip()
                    link = title_elem.get('href')
                    if not link.startswith('http'):
                        link = 'https://ria.ru' + link
                    
                    # –û–ø–∏—Å–∞–Ω–∏–µ
                    desc_elem = article.find('div', class_='list-item__announce')
                    description = desc_elem.text.strip()[:150] + '...' if desc_elem else self._generate_description(title)
                    
                    news_items.append({
                        'title': title,
                        'url': link,
                        'description': description
                    })
            
            return news_items
    
    # –ó–∞–≥–ª—É—à–∫–∏ –¥–ª—è –¥—Ä—É–≥–∏—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤
    async def parse_matchtv(self, url: str) -> List[Dict]:
        return await self._generic_parser(url, 'a', class_='news-card__title')
    
    async def parse_tjournal(self, url: str) -> List[Dict]:
        return await self._generic_parser(url, 'a', class_='content-title')
    
    async def parse_lenta(self, url: str) -> List[Dict]:
        return await self._generic_parser(url, 'a', class_='card-full-news__title')
    
    async def _generic_parser(self, url: str, tag: str, **kwargs) -> List[Dict]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä"""
        async with self.session.get(url) as response:
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            news_items = []
            elements = soup.find_all(tag, kwargs, limit=10)
            
            for elem in elements:
                title = elem.text.strip()
                link = elem.get('href')
                if link and not link.startswith('http'):
                    if 'lenta.ru' in url:
                        link = 'https://lenta.ru' + link
                
                news_items.append({
                    'title': title,
                    'url': link,
                    'description': self._generate_description(title)
                })
            
            return news_items
    
    def _generate_description(self, title: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        descriptions = [
            f"{title}. –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ —á–∏—Ç–∞–π—Ç–µ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–µ...",
            f"{title}. –≠—Ç–æ —Å–æ–±—ã—Ç–∏–µ –≤—ã–∑–≤–∞–ª–æ —à–∏—Ä–æ–∫–∏–π —Ä–µ–∑–æ–Ω–∞–Ω—Å...",
            f"{title}. –≠–∫—Å–ø–µ—Ä—Ç—ã –ø—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª–∏ —Å–∏—Ç—É–∞—Ü–∏—é...",
            f"{title}. –ß–∏—Ç–∞–π—Ç–µ –ø–æ–ª–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –ø–æ —Å—Å—ã–ª–∫–µ...",
            f"{title}. –ù–æ–≤–æ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è –≤ —Å–æ—Ü—Å–µ—Ç—è—Ö..."
        ]
        return random.choice(descriptions)
    
    # ========== –ü–ê–†–°–ò–ù–ì –ü–û–ì–û–î–´ ==========
    async def get_weather(self, city_name: str = "–ú–æ—Å–∫–≤–∞") -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–≥–æ–¥—É –¥–ª—è –≥–æ—Ä–æ–¥–∞"""
        cache_key = f"weather_{city_name}_{datetime.now().strftime('%Y%m%d%H')}"
        
        if cache_key in self.weather_cache:
            cached_time, data = self.weather_cache[cache_key]
            if (datetime.now() - cached_time).seconds < self.cache_timeout:
                return data
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–¥ –≥–æ—Ä–æ–¥–∞ –¥–ª—è –Ø–Ω–¥–µ–∫—Å
        city_data = next((c for c in CITIES if c['name'].lower() == city_name.lower()), CITIES[0])
        
        try:
            weather = await self._parse_yandex_weather(city_data['yandex_code'])
            weather['city'] = city_data['name']
            weather['updated'] = datetime.now().strftime('%H:%M')
            
            # –ö—ç—à–∏—Ä—É–µ–º
            self.weather_cache[cache_key] = (datetime.now(), weather)
            return weather
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ–≥–æ–¥—ã –¥–ª—è {city_name}: {e}")
            return self._get_fallback_weather(city_data['name'])
    
    async def _parse_yandex_weather(self, city_code: str) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–≥–æ–¥—ã —Å –Ø–Ω–¥–µ–∫—Å.–ü–æ–≥–æ–¥—ã"""
        url = f"https://yandex.ru/pogoda/{city_code}"
        
        headers = {
            'User-Agent': USER_AGENT.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        }
        
        async with self.session.get(url, headers=headers) as response:
            if response.status != 200:
                raise Exception(f"HTTP {response.status}")
            
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            weather = {}
            
            # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å–µ–π—á–∞—Å
            temp_elem = soup.find('span', class_='temp__value')
            if temp_elem:
                weather['temp_now'] = temp_elem.text.strip()
            
            # –û—â—É—â–∞–µ—Ç—Å—è –∫–∞–∫
            feels_label = soup.find('div', class_='term__label')
            if feels_label and '–æ—â—É—â–∞–µ—Ç—Å—è' in feels_label.text:
                feels_temp = feels_label.find_next('span', class_='temp__value')
                if feels_temp:
                    weather['feels_like'] = feels_temp.text.strip()
            
            # –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–≥–æ–¥—ã
            condition_elem = soup.find('div', class_='link__condition')
            if condition_elem:
                weather['condition'] = condition_elem.text.strip()
            
            # –í–µ—Ç–µ—Ä
            wind_elem = soup.find('span', class_='wind-speed')
            if wind_elem:
                weather['wind'] = wind_elem.text.strip()
            
            # –í–ª–∞–∂–Ω–æ—Å—Ç—å
            humidity_elem = soup.find('div', class_='term__label', text='–≤–ª–∞–∂–Ω–æ—Å—Ç—å')
            if humidity_elem:
                humidity = humidity_elem.find_next('div', class_='term__value')
                if humidity:
                    weather['humidity'] = humidity.text.strip()
            
            # –î–∞–≤–ª–µ–Ω–∏–µ
            pressure_elem = soup.find('div', class_='term__label', text='–¥–∞–≤–ª–µ–Ω–∏–µ')
            if pressure_elem:
                pressure = pressure_elem.find_next('div', class_='term__value')
                if pressure:
                    weather['pressure'] = pressure.text.strip()
            
            # –ï—Å–ª–∏ —á–µ–≥–æ-—Ç–æ –Ω–µ —Ö–≤–∞—Ç–∏–ª–æ, –∑–∞–ø–æ–ª–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            weather.setdefault('temp_now', '+5¬∞C')
            weather.setdefault('feels_like', '+3¬∞C')
            weather.setdefault('condition', '–û–±–ª–∞—á–Ω–æ —Å –ø—Ä–æ—è—Å–Ω–µ–Ω–∏—è–º–∏')
            weather.setdefault('wind', '3 –º/—Å')
            weather.setdefault('humidity', '75%')
            weather.setdefault('pressure', '755 –º–º —Ä—Ç.—Å—Ç.')
            
            return weather
    
    def _get_fallback_weather(self, city: str) -> Dict:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–≥–æ–¥–µ"""
        conditions = [
            "–Ø—Å–Ω–æ", "–û–±–ª–∞—á–Ω–æ", "–ù–µ–±–æ–ª—å—à–∞—è –æ–±–ª–∞—á–Ω–æ—Å—Ç—å", 
            "–ü–∞—Å–º—É—Ä–Ω–æ", "–ù–µ–±–æ–ª—å—à–æ–π –¥–æ–∂–¥—å", "–°–Ω–µ–≥"
        ]
        
        return {
            'city': city,
            'temp_now': f"+{random.randint(-5, 15)}¬∞C",
            'feels_like': f"+{random.randint(-7, 13)}¬∞C",
            'condition': random.choice(conditions),
            'wind': f"{random.randint(1, 10)} –º/—Å",
            'humidity': f"{random.randint(60, 90)}%",
            'pressure': f"{random.randint(740, 770)} –º–º —Ä—Ç.—Å—Ç.",
            'updated': datetime.now().strftime('%H:%M'),
            'source': '–∫—ç—à'
        }

# ========== –ö–õ–ê–°–° –î–õ–Ø –°–û–ó–î–ê–ù–ò–Ø –î–ê–ô–î–ñ–ï–°–¢–ê ==========
class DailyDigest:
    """–°–æ–∑–¥–∞–Ω–∏–µ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞"""
    
    def __init__(self):
        self.parser = NewsWeatherParser()
        self.emoji_map = {
            '—Å–ø–æ—Ä—Ç': '‚öΩ',
            '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': 'üíª',
            '–∫—É—Ä—å—ë–∑—ã': 'üòÇ'
        }
        self.category_names = {
            '—Å–ø–æ—Ä—Ç': '–ù–û–í–û–°–¢–¨ –°–ü–û–†–¢–ê',
            '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': '–¢–ï–•–ù–û–õ–û–ì–ò–ò –î–ù–Ø',
            '–∫—É—Ä—å—ë–∑—ã': '–ö–£–†–¨–Å–ó –î–ù–Ø'
        }
    
    async def create_digest(self) -> str:
        """–°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç"""
        try:
            await self.parser.init_session()
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –¥–∞—Ç–æ–π
            now = datetime.now(TIMEZONE)
            day_names = ["–ü–û–ù–ï–î–ï–õ–¨–ù–ò–ö", "–í–¢–û–†–ù–ò–ö", "–°–†–ï–î–ê", "–ß–ï–¢–í–ï–†–ì", "–ü–Ø–¢–ù–ò–¶–ê", "–°–£–ë–ë–û–¢–ê", "–í–û–°–ö–†–ï–°–ï–ù–¨–ï"]
            day_name = day_names[now.weekday()]
            date_str = now.strftime("%d.%m.%Y")
            
            digest = f"üåÖ –ï–ñ–ï–î–ù–ï–í–ù–´–ô –î–ê–ô–î–ñ–ï–°–¢ ‚Ä¢ {day_name}, {date_str}\n\n"
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –≤—Å–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            for category in ['—Å–ø–æ—Ä—Ç', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–∫—É—Ä—å—ë–∑—ã']:
                news_list = await self.parser.get_news_by_category(category, count=1)
                if news_list:
                    news = news_list[0]
                    digest += self._format_news_block(category, news)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–≥–æ–¥—É
            weather = await self.parser.get_weather("–ú–æ—Å–∫–≤–∞")
            digest += self._format_weather_block(weather)
            
            # –ü–æ–¥–ø–∏—Å—å
            digest += "\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
            digest += "üì± <i>–•–æ—Ä–æ—à–µ–≥–æ –¥–Ω—è! –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –±–æ—Ç–æ–º</i>"
            
            await self.parser.close_session()
            return digest
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–∞: {e}")
            return self._get_fallback_digest()
    
    def _format_news_block(self, category: str, news: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–ª–æ–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
        emoji = self.emoji_map.get(category, 'üì∞')
        category_title = self.category_names.get(category, category.upper())
        
        block = f"{emoji} {category_title}\n"
        block += f"üì∞ {news.get('title', '–ù–æ–≤–æ—Å—Ç—å –¥–Ω—è')}\n"
        block += f"{news.get('description', '–ß–∏—Ç–∞–π—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏...')}\n"
        
        if news.get('url'):
            # –°–æ–∫—Ä–∞—â–∞–µ–º –¥–æ–º–µ–Ω –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã
            source_name = news.get('source', '–ò—Å—Ç–æ—á–Ω–∏–∫')
            if 'sport-express.ru' in news['url']:
                source_name = '–°–ø–æ—Ä—Ç-–≠–∫—Å–ø—Ä–µ—Å—Å'
            elif 'habr.com' in news['url']:
                source_name = '–•–∞–±—Ä–∞—Ö–∞–±—Ä'
            elif 'kp.ru' in news['url']:
                source_name = '–ö–æ–º—Å–æ–º–æ–ª—å—Å–∫–∞—è –ø—Ä–∞–≤–¥–∞'
            
            block += f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}\n\n"
        
        return block
    
    def _format_weather_block(self, weather: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–ª–æ–∫–∞ –ø–æ–≥–æ–¥—ã"""
        # –≠–º–æ–¥–∑–∏ –¥–ª—è –ø–æ–≥–æ–¥—ã
        condition_emoji = {
            '—è—Å–Ω–æ': '‚òÄÔ∏è',
            '–æ–±–ª–∞—á–Ω–æ': '‚òÅÔ∏è',
            '–ø–∞—Å–º—É—Ä–Ω–æ': '‚òÅÔ∏è',
            '–¥–æ–∂–¥—å': 'üåßÔ∏è',
            '—Å–Ω–µ–≥': '‚ùÑÔ∏è',
            '–≥—Ä–æ–∑–∞': '‚õàÔ∏è'
        }
        
        condition = weather.get('condition', '').lower()
        emoji = 'üå§Ô∏è'
        for key, value in condition_emoji.items():
            if key in condition:
                emoji = value
                break
        
        block = f"{emoji} –ü–†–û–ì–ù–û–ó –ü–û–ì–û–î–´\n"
        block += f"üá∑üá∫ {weather.get('city', '–ú–æ—Å–∫–≤–∞')}\n"
        block += f"{emoji} {weather.get('condition', '–û–±–ª–∞—á–Ω–æ —Å –ø—Ä–æ—è—Å–Ω–µ–Ω–∏—è–º–∏')}\n"
        block += f"üå°Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {weather.get('temp_now', '+5¬∞C')}"
        
        if 'feels_like' in weather:
            block += f" (–æ—â—É—â–∞–µ—Ç—Å—è –∫–∞–∫ {weather['feels_like']})"
        
        block += f"\nüíß –í–ª–∞–∂–Ω–æ—Å—Ç—å: {weather.get('humidity', '75%')}\n"
        block += f"üí® –í–µ—Ç–µ—Ä: {weather.get('wind', '5 –º/—Å')}\n"
        block += f"üìä –î–∞–≤–ª–µ–Ω–∏–µ: {weather.get('pressure', '755 –º–º —Ä—Ç.—Å—Ç.')}\n\n"
        
        return block
    
    def _get_fallback_digest(self) -> str:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç"""
        now = datetime.now(TIMEZONE)
        day_names = ["–ü–û–ù–ï–î–ï–õ–¨–ù–ò–ö", "–í–¢–û–†–ù–ò–ö", "–°–†–ï–î–ê", "–ß–ï–¢–í–ï–†–ì", "–ü–Ø–¢–ù–ò–¶–ê", "–°–£–ë–ë–û–¢–ê", "–í–û–°–ö–†–ï–°–ï–ù–¨–ï"]
        day_name = day_names[now.weekday()]
        date_str = now.strftime("%d.%m.%Y")
        
        digest = f"üåÖ –ï–ñ–ï–î–ù–ï–í–ù–´–ô –î–ê–ô–î–ñ–ï–°–¢ ‚Ä¢ {day_name}, {date_str}\n\n"
        
        # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        fallback_news = [
            ("‚öΩ –ù–û–í–û–°–¢–¨ –°–ü–û–†–¢–ê", "–†–æ—Å—Å–∏–π—Å–∫–∏–µ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω—ã –ø–æ–∫–∞–∑–∞–ª–∏ –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è—Ö", "–°–ø–æ—Ä—Ç-–≠–∫—Å–ø—Ä–µ—Å—Å"),
            ("üíª –¢–ï–•–ù–û–õ–û–ì–ò–ò –î–ù–Ø", "–í –†–æ—Å—Å–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—É—é —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é –≤ —Å—Ñ–µ—Ä–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "–•–∞–±—Ä–∞—Ö–∞–±—Ä"),
            ("üòÇ –ö–£–†–¨–Å–ó –î–ù–Ø", "–ù–µ–æ–±—ã—á–Ω—ã–π —Å–ª—É—á–∞–π –ø—Ä–æ–∏–∑–æ—à—ë–ª —Å–µ–≥–æ–¥–Ω—è –≤ –æ–¥–Ω–æ–º –∏–∑ –≥–æ—Ä–æ–¥–æ–≤ –†–æ—Å—Å–∏–∏", "–ö–æ–º—Å–æ–º–æ–ª—å—Å–∫–∞—è –ø—Ä–∞–≤–¥–∞")
        ]
        
        for emoji, title, source in fallback_news:
            digest += f"{emoji}\nüì∞ {title}\n–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ —á–∏—Ç–∞–π—Ç–µ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö...\nüîó –ò—Å—Ç–æ—á–Ω–∏–∫: {source}\n\n"
        
        # –ü–æ–≥–æ–¥–∞
        digest += "üå§Ô∏è –ü–†–û–ì–ù–û–ó –ü–û–ì–û–î–´\nüá∑üá∫ –ú–æ—Å–∫–≤–∞\n‚òÅÔ∏è –û–±–ª–∞—á–Ω–æ —Å –ø—Ä–æ—è—Å–Ω–µ–Ω–∏—è–º–∏\nüå°Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: +5¬∞C (–æ—â—É—â–∞–µ—Ç—Å—è –∫–∞–∫ +3¬∞C)\nüíß –í–ª–∞–∂–Ω–æ—Å—Ç—å: 75%\nüí® –í–µ—Ç–µ—Ä: 5 –º/—Å\nüìä –î–∞–≤–ª–µ–Ω–∏–µ: 755 –º–º —Ä—Ç.—Å—Ç.\n\n"
        digest += "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nüì± <i>–•–æ—Ä–æ—à–µ–≥–æ –¥–Ω—è! –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –±–æ—Ç–æ–º</i>"
        
        return digest

# ========== –ö–õ–ê–°–° –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò ==========
class BotConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–æ—Ç–∞"""
    
    def __init__(self):
        self.data = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        if os.path.exists(CONFIG_FILE):
            try:
                with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞: {e}")
        
        return {
            "chat_id": None,
            "allowed_users": []
        }
    
    def save(self) -> None:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
    
    @property
    def chat_id(self) -> Optional[int]:
        return self.data.get("chat_id")
    
    @chat_id.setter
    def chat_id(self, value: int) -> None:
        self.data["chat_id"] = value
        self.save()
    
    @property
    def allowed_users(self) -> List[str]:
        return self.data.get("allowed_users", [])

# ========== –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –ë–û–¢–ê ==========
async def send_daily_digest(context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û—Ç–ø—Ä–∞–≤–∫–∞ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞"""
    try:
        config = BotConfig()
        chat_id = config.chat_id
        
        if not chat_id:
            logger.error("Chat ID –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
            await schedule_next_digest(context)
            return
        
        logger.info("–ù–∞—á–∏–Ω–∞—é —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç–∞...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞–π–¥–∂–µ—Å—Ç
        digest_creator = DailyDigest()
        message = await digest_creator.create_digest()
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º
        await context.bot.send_message(
            chat_id=chat_id,
            text=message,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True
        )
        
        logger.info("‚úÖ –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞: {e}")
    finally:
        # –ü–ª–∞–Ω–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –æ—Ç–ø—Ä–∞–≤–∫—É
        await schedule_next_digest(context)

async def schedule_next_digest(context: ContextTypes.DEFAULT_TYPE) -> None:
    """–ó–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –¥–∞–π–¥–∂–µ—Å—Ç"""
    try:
        now = datetime.now(TIMEZONE)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–µ–≥–æ–¥–Ω—è –ª–∏ –Ω—É–∂–Ω—ã–π –¥–µ–Ω—å –∏ –≤—Ä–µ–º—è —É–∂–µ –ø—Ä–æ—à–ª–æ
        if now.weekday() in DIGEST_DAYS:
            digest_time = now.replace(
                hour=DIGEST_TIME["hour"],
                minute=DIGEST_TIME["minute"],
                second=0,
                microsecond=0
            )
            
            if now < digest_time:
                delay = (digest_time - now).total_seconds()
                job_name = f"digest_{digest_time.strftime('%Y%m%d')}"
                schedule_job(context, delay, job_name)
                return
        
        # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–±–æ—á–∏–π –¥–µ–Ω—å
        days_ahead = 1
        while True:
            next_day = now + timedelta(days=days_ahead)
            if next_day.weekday() in DIGEST_DAYS:
                next_digest = next_day.replace(
                    hour=DIGEST_TIME["hour"],
                    minute=DIGEST_TIME["minute"],
                    second=0,
                    microsecond=0
                )
                delay = (next_digest - now).total_seconds()
                job_name = f"digest_{next_digest.strftime('%Y%m%d')}"
                schedule_job(context, delay, job_name)
                logger.info(f"–°–ª–µ–¥—É—é—â–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω –Ω–∞ {next_digest}")
                break
            days_ahead += 1
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–∞: {e}")
        # –ü—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞ —á–µ—Ä–µ–∑ —á–∞—Å
        context.application.job_queue.run_once(
            lambda ctx: schedule_next_digest(ctx),
            3600
        )

def schedule_job(context: ContextTypes.DEFAULT_TYPE, delay: float, name: str):
    """–ó–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞–Ω–∏–µ"""
    jobs = context.application.job_queue.jobs()
    if not any(j.name == name for j in jobs):
        context.application.job_queue.run_once(
            send_daily_digest,
            delay,
            name=name
        )

async def send_digest_now(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å –¥–∞–π–¥–∂–µ—Å—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ (–∫–æ–º–∞–Ω–¥–∞)"""
    config = BotConfig()
    chat_id = config.chat_id
    
    if not chat_id:
        await update.message.reply_text("‚ùå –°–Ω–∞—á–∞–ª–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —á–∞—Ç –∫–æ–º–∞–Ω–¥–æ–π /setchat")
        return
    
    await update.message.reply_text("üîÑ –°–æ–∑–¥–∞—é –¥–∞–π–¥–∂–µ—Å—Ç...")
    
    digest_creator = DailyDigest()
    message = await digest_creator.create_digest()
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ —Ü–µ–ª–µ–≤–æ–π —á–∞—Ç
    await context.bot.send_message(
        chat_id=chat_id,
        text=message,
        parse_mode=ParseMode.HTML,
        disable_web_page_preview=True
    )
    
    await update.message.reply_text("‚úÖ –î–∞–π–¥–∂–µ—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω!")

async def set_chat(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á–∞—Ç –¥–ª—è –¥–∞–π–¥–∂–µ—Å—Ç–∞"""
    chat_id = update.effective_chat.id
    chat_title = update.effective_chat.title or "–ª–∏—á–Ω—ã–π —á–∞—Ç"
    
    config = BotConfig()
    config.chat_id = chat_id
    
    await update.message.reply_text(
        f"‚úÖ <b>–ß–∞—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω:</b> {chat_title}\n"
        f"<b>Chat ID:</b> {chat_id}\n\n"
        f"–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å—Å—è –≤ —ç—Ç–æ—Ç —á–∞—Ç –≤ {DIGEST_TIME['hour']:02d}:{DIGEST_TIME['minute']:02d} –ø–æ –ú–°–ö (–ü–Ω-–ü—Ç)",
        parse_mode=ParseMode.HTML
    )
    
    # –ü–ª–∞–Ω–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–π –¥–∞–π–¥–∂–µ—Å—Ç
    await schedule_next_digest(context)

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–ö–æ–º–∞–Ω–¥–∞ /start"""
    await update.message.reply_text(
        "üåÖ <b>–ë–æ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞</b>\n\n"
        f"üì∞ <b>–ß—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –±–æ—Ç:</b>\n"
        f"‚Ä¢ –ù–æ–≤–æ—Å—Ç–∏ —Å–ø–æ—Ä—Ç–∞\n"
        f"‚Ä¢ –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏\n"
        f"‚Ä¢ –ö—É—Ä—å—ë–∑–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏\n"
        f"‚Ä¢ –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã –¥–ª—è –ú–æ—Å–∫–≤—ã\n\n"
        f"‚è∞ <b>–í—Ä–µ–º—è –æ—Ç–ø—Ä–∞–≤–∫–∏:</b> {DIGEST_TIME['hour']:02d}:{DIGEST_TIME['minute']:02d} –ø–æ –ú–°–ö (–ü–Ω-–ü—Ç)\n\n"
        f"üîß <b>–ö–æ–º–∞–Ω–¥—ã:</b>\n"
        f"/setchat - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á–∞—Ç –¥–ª—è —Ä–∞—Å—Å—ã–ª–∫–∏\n"
        f"/digestnow - –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –¥–∞–π–¥–∂–µ—Å—Ç —Å–µ–π—á–∞—Å\n"
        f"/info - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–æ—Ç–µ",
        parse_mode=ParseMode.HTML
    )

async def info(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–æ—Ç–µ"""
    config = BotConfig()
    chat_status = f"‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (ID: {config.chat_id})" if config.chat_id else "‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
    
    now = datetime.now(TIMEZONE)
    next_digest_time = calculate_next_digest_time()
    
    await update.message.reply_text(
        f"üìä <b>–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–æ—Ç–µ:</b>\n\n"
        f"üì± <b>–°—Ç–∞—Ç—É—Å —á–∞—Ç–∞:</b> {chat_status}\n"
        f"‚è∞ <b>–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ:</b> –ü–Ω-–ü—Ç –≤ {DIGEST_TIME['hour']:02d}:{DIGEST_TIME['minute']:02d} –ú–°–ö\n"
        f"‚û°Ô∏è <b>–°–ª–µ–¥—É—é—â–∏–π –¥–∞–π–¥–∂–µ—Å—Ç:</b> {next_digest_time.strftime('%d.%m.%Y –≤ %H:%M')}\n\n"
        f"üì∞ <b>–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π:</b>\n"
        f"‚Ä¢ –°–ø–æ—Ä—Ç-–≠–∫—Å–ø—Ä–µ—Å—Å, –ß–µ–º–ø–∏–æ–Ω–∞—Ç\n"
        f"‚Ä¢ –•–∞–±—Ä–∞—Ö–∞–±—Ä, VC.ru\n"
        f"‚Ä¢ –ö–æ–º—Å–æ–º–æ–ª–∫–∞, –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏\n\n"
        f"üå§Ô∏è <b>–ü–æ–≥–æ–¥–∞:</b> –Ø–Ω–¥–µ–∫—Å.–ü–æ–≥–æ–¥–∞ (–ú–æ—Å–∫–≤–∞)",
        parse_mode=ParseMode.HTML
    )

def calculate_next_digest_time() -> datetime:
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –≤—Ä–µ–º—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞"""
    now = datetime.now(TIMEZONE)
    
    if now.weekday() in DIGEST_DAYS:
        digest_time = now.replace(
            hour=DIGEST_TIME["hour"],
            minute=DIGEST_TIME["minute"],
            second=0,
            microsecond=0
        )
        if now < digest_time:
            return digest_time
    
    # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–±–æ—á–∏–π –¥–µ–Ω—å
    days_ahead = 1
    while True:
        next_day = now + timedelta(days=days_ahead)
        if next_day.weekday() in DIGEST_DAYS:
            return next_day.replace(
                hour=DIGEST_TIME["hour"],
                minute=DIGEST_TIME["minute"],
                second=0,
                microsecond=0
            )
        days_ahead += 1

def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    if not TOKEN:
        logger.error("‚ùå –¢–æ–∫–µ–Ω –±–æ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ TELEGRAM_BOT_TOKEN")
        return
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º uvloop –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    try:
        import uvloop
        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
        logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è uvloop –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏")
    except ImportError:
        logger.warning("‚ö†Ô∏è  uvloop –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install uvloop")
    
    application = Application.builder().token(TOKEN).build()
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("setchat", set_chat))
    application.add_handler(CommandHandler("digestnow", send_digest_now))
    application.add_handler(CommandHandler("info", info))
    
    # –ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
    application.job_queue.run_once(
        lambda ctx: schedule_next_digest(ctx),
        3
    )
    
    logger.info("ü§ñ –ë–æ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞ –∑–∞–ø—É—â–µ–Ω!")
    logger.info(f"‚è∞ –î–∞–π–¥–∂–µ—Å—Ç—ã: –ü–Ω-–ü—Ç –≤ {DIGEST_TIME['hour']:02d}:{DIGEST_TIME['minute']:02d} –ø–æ –ú–°–ö")
    logger.info(f"üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {len(NEWS_SOURCES['—Å–ø–æ—Ä—Ç']) + len(NEWS_SOURCES['—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏']) + len(NEWS_SOURCES['–∫—É—Ä—å—ë–∑—ã'])} —Å–∞–π—Ç–æ–≤")
    
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == "__main__":
    main()
